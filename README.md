# Mod√©lisation de sujets (LDA / NMF / BERTopic) + Visualisationn (Streamlit)

[Voir le d√©p√¥t sur GitHub](https://github.com/saiken77/topic-modeling-pipeline-fr-app)

R√©sum√©
Ce d√©p√¥t contient un pipeline complet pour **pr√©traiter** un corpus d'articles, **extraire des sujets** avec trois m√©thodes (LDA, NMF, BERTopic), **comparer** les r√©sultats et **visualiser / √©diter** les topics via une application Streamlit. Le produit final est un unique fichier JSON `results/topics.json` mis √† jour **progressivement** (apr√®s LDA, apr√®s NMF, apr√®s BERTopic, puis apr√®s calcul de coh√©rence si demand√©).

---

## Arborescence du d√©p√¥t (raccourci)

```
topic-modeling-pipeline-fr-app/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # corpus brut (articles.csv attendu)
‚îÇ   ‚îî‚îÄ‚îÄ corpus_clean.csv      # corpus nettoy√© (g√©n√©r√© par le pipeline)
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ topics.json           # fichier unique de sortie (lda,nmf,bertopic,coherence,_meta)
‚îÇ   ‚îî‚îÄ‚îÄ backups/              # backups automatiques (√©crasement safe)
‚îÇ
‚îú‚îÄ‚îÄ app.py                    # application Streamlit pour visualiser / √©diter / relancer pipeline
‚îú‚îÄ‚îÄ topic_pipeline.py         # script principal CLI (pr√©traitement + mod√®les + sauvegarde progressive)
‚îú‚îÄ‚îÄ utils_defaults.py         # fonctions default_n_topics / default_ber_min_topic_size
‚îî‚îÄ‚îÄ requirements.txt          # (recommand√©) d√©pendances Python
```

---

## Pr√©requis

Recommand√© : cr√©er un environnement virtuel (conda / venv). Exemple (conda) :

```bash
conda create -n nlp39 python=3.9 -y
conda activate nlp39
```

Installer les d√©pendances (adapt√© selon ton gestionnaire) :

```bash
pip install -r requirements.txt
```

**Packages cl√©s (exemples)** :

* `spacy`
* `fr-core-news-sm` (mod√®le spaCy fran√ßais)
* `scikit-learn`
* `gensim`
* `bertopic`
* `sentence-transformers`
* `pandas`, `numpy`
* `streamlit`
* `wordcloud`, `matplotlib`, `altair`

Installer le mod√®le spaCy (obligatoire) :

```bash
python -m spacy download fr_core_news_sm
```

**Remarques :**

* BERTopic et sentence-transformers t√©l√©chargent/p√®sent des mod√®les ; pr√©voir de la RAM et un peu de temps.
* Si tu utilises un GPU, configurer les packages compatibles (optionnel).

---

## Fichiers importants & r√¥le

* `topic_pipeline.py`
  Script CLI principal. Il :

  * g√©n√®re `data/corpus_clean.csv` √† partir de `data/raw/articles.csv` si absent (option `--preprocess` pour forcer),
  * ex√©cute LDA, NMF, BERTopic (modes `--run`),
  * sauvegarde **progressivement** `results/topics.json` apr√®s chaque mod√®le,
  * calcule la coh√©rence `c_v` uniquement si `--compute-coherence` est pass√© (ou en mode `--run coherence`),
  * met √† jour `_meta` dans `results/topics.json` (timestamps ISO) pour indiquer la fra√Æcheur.
    Voir section ¬´ Utilisation ¬ª pour la liste compl√®te d'options.

* `utils_defaults.py`
  Contient :

  * `default_n_topics(n_docs)` ‚Äî heuristique dynamique pour `n_topics` (LDA/NMF),
  * `default_ber_min_topic_size(n_docs)` ‚Äî heuristique pour `min_topic_size` de BERTopic.
    Ces fonctions sont utilis√©es par le pipeline et par l'interface Streamlit pour fixer les valeurs par d√©faut dynamiques.

* `app.py`
  Application web interactive pour :

  * visualiser topics (BERTopic / LDA / NMF),
  * √©diter (renommer topics, d√©placer documents entre topics du m√™me mod√®le, fusionner topics),
  * relancer le pipeline localement (en passant des param√®tres),
  * recalculer uniquement la coh√©rence,
  * sauvegarder les changements (backup automatique).

* `results/topics.json` (format attendu) :
  Exemple simplifi√© :

  ```json
  {
    "lda": [
      {"topic_id": 0, "keywords": ["mot1","mot2"], "documents": [1,5], "label":"mot1 mot2 mot3"},
      ...
    ],
    "nmf": [ ... ],
    "bertopic": [ ... ],
    "coherence": {"lda": 0.32, "nmf": 0.28, "bertopic": 0.35},
    "_meta": {"lda": "2025-09-26T12:00:00", "nmf": "...", "bertopic":"...", "coherence": "..."}
  }
  ```

---

## Utilisation ‚Äî commandes essentielles

> **Pr√©paration** : assure-toi que `data/raw/articles.csv` existe et contient au minimum les colonnes `titre`/`texte` (ou `title`/`text`).

### 1) G√©n√©rer le corpus propre (si absent ou pour forcer)

```bash
python topic_pipeline.py --preprocess
```

Ce script va :

* charger `data/raw/articles.csv`,
* appliquer le nettoyage / lemmatisation via spaCy (`fr_core_news_sm`),
* produire `data/corpus_clean.csv`.

### 2) Ex√©cuter pipeline (sans coh√©rence ‚Äî plus rapide)

```bash
python topic_pipeline.py --run all
```

### 3) Ex√©cuter pipeline et calculer la coh√©rence (plus long)

```bash
python topic_pipeline.py --run all --compute-coherence
# ou pour n'ex√©cuter QUE la coh√©rence (apr√®s avoir g√©n√©r√© topics.json) :
python topic_pipeline.py --run coherence
```

### 4) Ex√©cution cibl√©e

* LDA uniquement :

  ```bash
  python topic_pipeline.py --run lda --n_topics_lda 8
  ```
* NMF uniquement :

  ```bash
  python topic_pipeline.py --run nmf --n_topics_nmf 6
  ```
* LDA + NMF (lda_nmf) :

  ```bash
  python topic_pipeline.py --run lda_nmf
  ```
* BERTopic (n√©cessite `--ber_min_topic_size` entier, sinon la valeur par d√©faut heuristique est utilis√©e) :

  ```bash
  python topic_pipeline.py --run bertopic --ber_min_topic_size 5
  ```

### 5) Lancer l'interface Streamlit

```bash
streamlit run app.py
```

Interface :

* s√©lectionner le mod√®le (BERTopic / LDA / NMF),
* visualiser les topics, mots-cl√©s, wordcloud et documents,
* renommer / d√©placer / fusionner topics (modifications en session, puis `Sauvegarder modifications` pour persister),
* relancer le pipeline local via la sidebar (param√®tres dynamiques calcul√©s depuis `utils_defaults`),
* bouton ¬´ Calculer uniquement la coh√©rence ¬ª si besoin.

---

## Exemple d'ex√©cution rapide (step-by-step)

```bash
# 1) Cr√©er l'environnement & installer d√©pendances
conda create -n nlp39 python=3.9 -y
conda activate nlp39
pip install -r requirements.txt
python -m spacy download fr_core_news_sm

# 2) G√©n√©rer corpus propre (si n√©cessaire)
python topic_pipeline.py --preprocess

# 3) Lancer pipeline (rapide sans coh√©rence)
python topic_pipeline.py --run all

# 4) Lancer Streamlit
streamlit run app.py
```

---

## D√©pannage / FAQ

* **Erreur `fr_core_news_sm` introuvable**
  ‚Üí ex√©cuter : `python -m spacy download fr_core_news_sm`

* **Longtemps bloqu√© au calcul de la coh√©rence**
  ‚Üí relancer sans coh√©rence : `python topic_pipeline.py --run all --no-coherence` (ou `--run all` sans `--compute-coherence`), puis lancer la coh√©rence s√©par√©ment lorsqu'on est pr√™t : `python topic_pipeline.py --run coherence`.

* **Probl√®mes avec BERTopic / sentence-transformers**

  * Ces √©tapes n√©cessitent souvent plus de m√©moire et parfois l‚Äôacc√®s internet la premi√®re fois (t√©l√©chargement de mod√®les). Si l‚Äô√©tape bloque, v√©rifier la RAM/disque et relancer avec `--run bertopic` seul pour mieux suivre l‚Äôavancement.

* **Streamlit ne montre pas les modifications imm√©diates**

  * L‚Äôapp stocke les modifications en `st.session_state`. Clique sur `Sauvegarder modifications` pour persister sur disque. Si tu veux annuler, recharger l‚Äôapp ou utiliser le bouton pour recharger le pipeline.

* **Param√®tre `stop_words='french'` (erreur sklearn)**

  * Le pipeline utilise la conversion explicite des stopwords spaCy en liste compatible sklearn pour √©viter ce probl√®me.

---

## Auteurs

Projet r√©alis√© dans le cadre du module **Natural Language Processing (NLP)** ‚Äî M1/S2 Fouilles de Donn√©es & Intelligence Artificielle (Universit√© Virtuelle du Burkina Faso).

- **SAWADOGO Abdel Sa√Ød Najib**
- **COULIBALY Cheick Ahmed**
- **BAZIE Dureel**

---

## Encadrement

- **Dr. Rodrique KAFANDO** ‚Äî Responsable p√©dagogique du module NLP

---

## üìú Licence

Ce projet est sous licence MIT. Voir [LICENSE](LICENCE) pour plus de d√©tails.